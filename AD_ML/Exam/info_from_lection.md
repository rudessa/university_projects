Данные:
- упорядоченные (множества), неупорядоченные (векторов), частично упорядоченные (одномерные или временные)

Классификация по типу шкалы:
- Категориальные
  - Номинальные
  - Порядковые
- Числовые

Регрессия - метод с учителем

Интерполяция - заполнение данных

Аппроксимация - замена на модель

Прореживание

Распределения:
- Дискретные (bins = кол-во состояний):
  - Бернулли
  - Биномиальное
  - Пуассона
- Непрерывные (bins = n**0.5):
  - Uniform
  - Нормальное (проверить критерием Шапиро)
  - Student
  - $χ^2$

Доверительный интервал:
- Двухсторонний - отсечь $α/2$ с двух сторон
- Односторонний

Гипотезы:
- Основная H0
- Альтернативная H1

Критерии:
- Параметрические (среднее, СКО)
  - T (если СКО неизвестно, статистики распределены по стьюденту)
   - степени свободы = кол-во элементов - кол-во переменных
  - Z (статистики распределены нормально)
- Непараметрические
  - Перестановочные (Манна-Уитни и т.д.)
  - Бутстрап

Уровень значимости выбирается до анализа

Бонферонни от метода Бенджамина-Хохберга

Тестировать только одну гипотезу

Увеличение (полиномиальные фичи) / понижение (лин PCA/SVD, нелин TSNE, UMAP) размерностей данных

Кластеризация:
- Метрические (расстояния)
  - DBSCAN
  - Spectral
- Модельные (координаты)
  - Gaussian Mixture

Критерии:
- Внешние (осмотр)
- Внутренние (по выборке и результатам)
  - BIC (где известно кол-во внутренних параметров) - минимизирует подобие между данными
  - Silhouette (если можно охарактеризовать сферами)


Линейная регрессия


Регуляризация:
- L1 (зануление коэффициентов) Lasso
- L2 (понижение их значения) Ridge, по-Тихонову

MAE более устойчива к выбросам, чем MSE


## Линейная регрессия
метод наименьших квадратов

$|y_i-y_m|^2→min$

$MSE=∑(oy)^2$ - только с нормальным

Проверить, что ошибка распределена нормально:

- QQ - график почти диагональный

- Метод Шапира-Уилка

Иначе есть выбросы:

Один нейрон с линейной функцией

Использовать $MAE=∑|Δy|$ - устойчива к выбросам

Любая метрика Меньковского

$ρ=(∑|Δy|^2)^p$

при o < p < 1

## Методы регуляризации
L1 Lasso ~ MAE $∑(A_i)$ - занулить один из коэффициентов

подбирается лямбда в $λR$

L2 Ridge ~ Loss $∑()$ - минимализировать коэффициенты

## Кластеризация - разделение объектов на группы похожих объектов
У каждой есть по крайней мере один гиперпараметр

Жёсткая - принадлежность одному кластеру

predict

Мягкая (fuzzy) - вероятность принадлежности ко всем кластерам

predict_proba

Плоские методы - выдаёт одну кластеризацию

Иерархические методы - выдаёт набор кластеризаций, один параметр - n_clusters
- аггломеративные - каждая точка - кластер в начале, каждый этап ближайшие соединяются
- дивизионные - один большой кластер в начале, каждый раз наибольший делится пополам c помощью KMeans


Метрическая - расстояния между объектами
- DBSCAN
- spectral
- optics
- KMeans

Плюсы:
- можеть создавать кластера произвольной формы
- не нужно знать положения объектов

Минусы:
- неустойчивость к шумам

Модельная - положение объектов в пространстве, точки удовлетворяют какому-либо распределению
- Gaussian Mixture - точки удовлетворяют нормальному распределению

Плюсы:
- устойчивы к шумам
- возможность мягкой кластеризации - вероятность принадлежности точек к какому-либо из кластеров

Минусы:
- форма фиксированна


### Подбор гиперпараметров
Внешний - подобрать с помощью эксперта

Внутренний - использовать критерий
- silhouette - считает, что данные распределены внутри сфер, отношение между границами и центрами кластеров $S = l/L$, ищем максимальный
 - любые кластеризации
- BIC - произведение независимых вероятностей  $-logL$ - логарифм правдоподобия Фишера, функция правдоподобия, максимум = 1, добавляют линейную функцию $αN$, ищем минимальный
 - Известно количество параметров, модельные кластеризации
- Акаике AIC



Тематическое моделирование

> гиперпараметры: число тем

Мешок слов - набор слов текста с вероятностями, получается матрица из чисел - эмбеддингов слов, при вращении обычно приводят к блочно-диагональному виду, получаются темы

LSA - Latent Stohastic Analysis - перестановка строк и столбцов

LDA - Latent Dirichlet Analysis - распределение дирихле, фитируются параметры моделей
> на матрицу смотрят как на вероятность $P(W,D)=∑P(T)P(W|T)P(D|T)$, мультиномиальное распределение

$P(n_1,n_2,n_3)=n!p_1^{n_1}p_2^{n_2}p_3^{n_3}/n_1!n_2!n_3!$

$A=U\Sigma V^{-x}$

$\sum (A_{ij}-A^M_{ij})^2→min$
+регуляризаторы
+$λ|U|+$

\> 0 разделяет темы

< 0 объединяет темы

TF - text freq

DF - количество документов

$TF/DF = TFIDF$


## Матричные преобразования
Хаар - преобразование x, y типа полусумма и полу разность

Уолша - составляется из матриц предыдущей степени из матриц Хаара, получатся новые величины - спектр Уолша

Фурье - в качестве базовых функций выступают синусы и косинусы

>прямое

$C_ω=∫u(t)e ^{-iωt}$

ортогональность



>обратное

$u(t)=∫C(ω)e ^{-iωt}$

cas = cos + sin
Быстрое преобразование Фурье

## Методы спектральной оценки
Оконное преобразование Фурье - взять оконную функцию по функции и только потом преобразование Фурье, листок расширяется
- окно Кайзера - лучшее
- окно Блэкмэна


## Параметрические методы оценивания
Подавляют боковые лепестки

ARIMA
$y_i=∑_{j=i}^∞ A_jy_{i-j} \leftarrow AR + ∑_{j=i}^∞ B*\Sigma_{i-j} \leftarrow MA$

Стационарность

Продиффиренцировав можно сделать стационарным $y_i=y_i-y_{i-1}$, результаты нужно проинтегрировать


# ВЫБРОСЫ
- Глобальные - значение очень сильно отличается
- Локальные - амплитуда значительно меньше
- Контекстные - несколько значений идут подряд и отличаются от других в системе

Способы устранения:
- удаление
> строится доверительный интервал по $\alpha = 0.05$ , за его пределами выбросы

- автоэнкодер
> бутылочное горлышко из малого количества нейронов, выход = вход = все точки, первая сеть енкодер, вторая - декодер; рекомендуется метрика MAE, меньше - обычные, больше - необычные; fit(x,x)


## Повышение и понижение размерностей
Проклятие размерности - при большой размерности и малом количестве сэмплов расстояния становятся почти неразличимыми

$log_2N < размерность < N^{0.5}$, N - сэмплы
Повышение
- Спрямляющее пространство
> Подобрать функции, использовать полиномиальные фичи; степень полинома больше двух не рекомендуется

Понижение
- Линейные
 - PCA
- Нелинейные - для кластеризации, близкие сближаются, далёкие отдаляются
 - t-SNE - считает, что пространство евклидово
 - UMAP - считает, что пространство только локально евклидово

 # Задача оттока
 Метод интервалов - из последовательностей выделить доверительные интервалы изменения величин, если они не пересекаются - они различные, иначе однозначного вывода сделать нельзя

Метод оттока
- работа с категориальными переменными
> посчитать вероятность оттока, доверительный интервал, вероятность при разных параметрах, при пересечении интервалов ничего нельзя сказать, иначе выброс и нужно анализировать как выброс, выше - чаще, ниже - реже; Z для доли $Z=p_1-p_0/p_0(1-p_0)$ pvalue равны или не равны, нужно скорректировать Бенджамини-Хофбергом
- числовые
> дискретизировать
 - эквидистантная - бьётся на группы ширины длины N, пишутся кодовые числа вместо некатегориальных переменных
 - квантильная - в каждой группе примерно одинаковое число людей, ширины неодинаковые

 Для каждого столбца есть вероятность, они перемножаются при предположении, что категории независимы

 ## Корреляция - мера взаимосвязи
 - Линейная
 - Пирсон
 > y = Ax + 𝜺 \
 > $R_П$ [-1,1] R=0 - корреляции нет
- Ранговая
 - Spearman - вытаскивает монотонные нелинейные зависимости
 > как Rank y зависит от Rank x, проверяется как они коррелируются по Пирсону [-1,1]

 Рекомендуется считать оба, если примерно равны, то скорее всего есть линейная корреляция, если Пирсон меньше, то корреляция скорее всего есть

 Задачи:
 - есть или нет? - анализ полученного pvalue - значимость
 - какая? - - уровень значимости, размер эффекта


## Поиск правил - задача рекомендаций
- Apriory - берутся все товары и берутся наиболее значимые, с вероятностью большей, чем заданная, потом строятся пары и группы товаров, потом условная веротяность (A&B|C) > α
> минимальная вероятность купить товар, группу товаров
- FP-tree - каждому уровню соответсвует товар по убыванию вероятности


