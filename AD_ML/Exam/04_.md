
# Билет №4(здесь мало инфы про регуляризацию и борьбу с выбросами)
4. Линейная регрессия, метод наименьших квадратов, регуляризация, борьба с выбросами
## 4.1. [Метод наименьших квадратов. Коэффициент детерминации. Сравнение двух (и более) уравнений линейной регрессии.]()
## 4.2. [Множественная регрессия. Тестирование гипотез для множественной регрессии. Коэффициент детерминации. Мультиколлинеарность.](#34-множественная-регрессия-тестирование-гипотез-для-множественной-регрессии-коэффициент-детерминации-мультиколлинеарность)
## 4.3. [Анализ остатков регрессии. Критерий Дарбина-Уотсона. Критерий Харке-Бера. Критерий Бройша-Пагана. Требования к выборке для проведения регрессионного анализа.](#35-анализ-остатков-регрессии-критерий-дарбина-уотсона-критерий-харке-бера-критерий-бройша-пагана-требования-к-выборке-для-проведения-регрессионного-анализа)
## 4.4. [Требования к выборке для проведения регрессионного анализа. Нелинейная регрессия. ANCOVA.](#36-требования-к-выборке-для-проведения-регрессионного-анализа-нелинейная-регрессия-ancova)




# Билет №4(здесь мало инфы про регуляризацию и борьбу с выбросами)
## 4. Линейная регрессия, метод наименьших квадратов, регуляризация, борьба с выбросами

# ---

**Теория:**

- Линейная регрессия:
  \( y = \beta_0 + \beta_1 x + \epsilon \)
  
- Метод наименьших квадратов:
  - Минимизирует сумму квадратов отклонений.

- Регуляризация:
  - L1-регуляризация (Lasso)
  - L2-регуляризация (Ridge)

**Код:**

```python
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Генерация данных
X = np.random.rand(100, 1) * 10
y = 3 * X.squeeze() + np.random.randn(100) * 3

# Линейная регрессия
model = LinearRegression()
model.fit(X, y)

# Прогноз
y_pred = model.predict(X)

plt.scatter(X, y, alpha=0.6, label="Данные")
plt.plot(X, y_pred, color='r', label="Модель")
plt.legend()
plt.show()

# Регуляризация
ridge = Ridge(alpha=1.0)
ridge.fit(X, y)
ridge_pred = ridge.predict(X)

plt.scatter(X, y, alpha=0.6, label="Данные")
plt.plot(X, ridge_pred, color='g', label="Ridge")
plt.legend()
plt.show()
```
# ---

## 4.1. Метод наименьших квадратов. Коэффициент детерминации. Сравнение двух (и более) уравнений линейной регрессии.

### Метод наименьших квадратов

Метод наименьших квадратов (МНК) используется для нахождения параметров линейной регрессии, минимизируя сумму квадратов отклонений наблюдаемых значений от предсказанных. 

#### Основная формула

Для линейной регрессии вида:

$$y = \beta_0 + \beta_1 x + \epsilon$$

где:
- $y$ — зависимая переменная,
- $x$ — независимая переменная,
- $\beta_0$ и $\beta_1$ — коэффициенты регрессии,
- $\epsilon$ — ошибка.

Метод наименьших квадратов минимизирует сумму квадратов ошибок:

$$S = \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2$$

Путем дифференцирования $S$ по $\beta_0$ и $\beta_1$ и приравнивания производных к нулю, получаем следующие нормальные уравнения:

$$\sum_{i=1}^{n} y_i = n\beta_0 + \beta_1 \sum_{i=1}^{n} x_i$$

$$\sum_{i=1}^{n} x_i y_i = \beta_0 \sum_{i=1}^{n} x_i + \beta_1 \sum_{i=1}^{n} x_i^2$$

Решив систему этих уравнений, мы находим $\beta_0$ и $\beta_1$.

#### Коэффициент детерминации ($R^2$)

Коэффициент детерминации $R^2$ измеряет долю вариации зависимой переменной, объясняемой регрессионной моделью.

$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$$

где:
- $SS_{res}$ — сумма квадратов остатков (ошибок),
- $SS_{tot}$ — общая сумма квадратов (вариация зависимой переменной).

$$SS_{res} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

$$SS_{tot} = \sum_{i=1}^{n} (y_i - \bar{y})^2$$

Значение $R^2$ варьируется от 0 до 1. Чем ближе значение $R^2$ к 1, тем лучше модель объясняет вариацию данных.

### Сравнение двух (и более) уравнений линейной регрессии

Для сравнения двух или более уравнений линейной регрессии используются различные подходы, включая сравнение коэффициентов регрессии и тестирование значимости различий между моделями.

#### 1. Сравнение коэффициентов регрессии

Сравнение коэффициентов регрессии между двумя моделями можно осуществить с помощью теста гипотез. Например, для проверки гипотезы о равенстве двух коэффициентов $\beta_1$ и $\beta_2$ из двух моделей:

$$H_0: \beta_{1,1} = \beta_{1,2}$$

$$H_1: \beta_{1,1} \neq \beta_{1,2}$$

#### 2. Тестирование значимости различий между моделями

Для проверки значимости различий между моделями можно использовать F-тест:

$$F = \frac{(SS_{res1} - SS_{res2}) / (df_1 - df_2)}{SS_{res2} / df_2}$$

где:
- $SS_{res1}$ и $SS_{res2}$ — сумма квадратов остатков для первой и второй модели,
- $df_1$ и $df_2$ — степени свободы для первой и второй модели.

Если значение $F$ превышает критическое значение для заданного уровня значимости, нулевая гипотеза отвергается, что указывает на значимые различия между моделями.

#### Пример

Предположим, у нас есть данные о продажах ($y$) и рекламе ($x$) для двух разных регионов. Мы строим две модели линейной регрессии для каждого региона:

Модель 1: $y_1 = \beta_{0,1} + \beta_{1,1} x + \epsilon_1$

Модель 2: $y_2 = \beta_{0,2} + \beta_{1,2} x + \epsilon_2$

1. **Расчет коэффициентов регрессии:**
   Используя метод наименьших квадратов, находим $\beta_{0,1}, \beta_{1,1}$ для первой модели и $\beta_{0,2}, \beta_{1,2}$ для второй модели.

2. **Вычисление $R^2$:**
   Вычисляем $R^2$ для обеих моделей, чтобы оценить, какая модель лучше объясняет вариацию данных.

3. **Сравнение моделей:**
   Применяем F-тест для проверки значимости различий между двумя моделями. Если нулевая гипотеза отвергается, это указывает на значимые различия между двумя регрессионными моделями.

Этот процесс позволяет оценить, насколько хорошо каждая модель описывает данные и существуют ли значимые различия между различными регрессионными моделями.

[Вернуться к вопросам](#ответы-на-вопросы-по-теории)

## 4.2. Множественная регрессия. Тестирование гипотез для множественной регрессии. Коэффициент детерминации. Мультиколлинеарность.

### Множественная регрессия

Множественная регрессия расширяет простую линейную регрессию, включив в модель несколько независимых переменных. Формула множественной регрессии выглядит следующим образом:

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k + \epsilon$$

где:
- $y$ — зависимая переменная,
- $x_1, x_2, \ldots, x_k$ — независимые переменные,
- $\beta_0, \beta_1, \beta_2, \ldots, \beta_k$ — коэффициенты регрессии,
- $\epsilon$ — ошибка.

### Тестирование гипотез для множественной регрессии

#### Тестирование значимости модели

Нулевая гипотеза ($H_0$): Все коэффициенты $\beta_1, \beta_2, \ldots, \beta_k$ равны нулю.
Альтернативная гипотеза ($H_1$): По крайней мере, один из коэффициентов $\beta_i$ не равен нулю.

Для тестирования этой гипотезы используется F-тест:

$$F = \frac{(SSR / k)}{(SSE / (n - k - 1))}$$

где:
- $SSR$ (Sum of Squares Regression) — сумма квадратов регрессии,
- $SSE$ (Sum of Squares Error) — сумма квадратов ошибок,
- $k$ — число независимых переменных,
- $n$ — общее число наблюдений.

Если $F$ превышает критическое значение, нулевая гипотеза отвергается, что означает, что модель значима.

#### Тестирование значимости отдельных коэффициентов

Нулевая гипотеза ($H_0$): Коэффициент $\beta_i = 0$.
Альтернативная гипотеза ($H_1$): Коэффициент $\beta_i \neq 0$.

Для тестирования этой гипотезы используется t-тест:

$$t = \frac{\hat{\beta}_i}{SE(\hat{\beta}_i)}$$

где $\hat{\beta}_i$ — оценка коэффициента $\beta_i$, $SE(\hat{\beta}_i)$ — стандартная ошибка коэффициента.

Если $|t|$ превышает критическое значение, нулевая гипотеза отвергается, что означает, что данный коэффициент значим.

### Коэффициент детерминации ($R^2$)

Коэффициент детерминации $R^2$ измеряет долю вариации зависимой переменной, объясняемой регрессионной моделью:

$$R^2 = 1 - \frac{SSE}{SST}$$

где:
- $SSE$ (Sum of Squares Error) — сумма квадратов ошибок,
- $SST$ (Total Sum of Squares) — общая сумма квадратов.

Значение $R^2$ варьируется от 0 до 1. Чем ближе значение $R^2$ к 1, тем лучше модель объясняет вариацию данных.

### Мультиколлинеарность

Мультиколлинеарность возникает, когда независимые переменные в модели сильно коррелированы друг с другом. Это может привести к нестабильности оценок коэффициентов регрессии и затруднить интерпретацию результатов.

#### Признаки мультиколлинеарности

1. **Высокие значения коэффициентов корреляции** между независимыми переменными.
2. **Высокие стандартные ошибки** коэффициентов регрессии.
3. **Низкие значения t-статистики** для значимых переменных.
4. **Высокие значения показателя VIF (Variance Inflation Factor)**:

   $$VIF = \frac{1}{1 - R_j^2}$$

   где $R_j^2$ — коэффициент детерминации для регрессии $x_j$ на остальные независимые переменные. Если $VIF > 10$, это указывает на серьезную мультиколлинеарность.

#### Методы борьбы с мультиколлинеарностью

1. **Удаление одной из коррелированных переменных.**
2. **Снижение размерности данных** с помощью методов, таких как анализ главных компонент (PCA).
3. **Регуляризация** (например, Ridge регрессия или Lasso регрессия), которая добавляет штраф за большие значения коэффициентов.

### Пример множественной регрессии

Предположим, у нас есть данные о продажах ($y$), рекламе на телевидении ($x_1$) и рекламе в интернете ($x_2$). Мы хотим построить модель множественной регрессии, чтобы предсказать продажи на основе рекламы.

1. **Модель:**

   $$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$$

2. **Оценка коэффициентов:**
   Используя метод наименьших квадратов, находим $\beta_0, \beta_1, \beta_2$.

3. **Тестирование гипотез:**
   Проверяем значимость модели и отдельных коэффициентов с помощью F-теста и t-тестов.

4. **Вычисление $R^2$:**
   Оцениваем, насколько хорошо модель объясняет вариацию данных.

5. **Проверка на мультиколлинеарность:**
   Рассчитываем VIF для независимых переменных и проверяем наличие мультиколлинеарности.

6. **Интерпретация результатов:**
   Анализируем значимость переменных и их вклад в предсказание продаж.

Этот процесс позволяет построить и оценить модель множественной регрессии, а также принять меры для улучшения ее качества при необходимости.

[Вернуться к вопросам](#ответы-на-вопросы-по-теории)

## 4.3. Анализ остатков регрессии. Критерий Дарбина-Уотсона. Критерий Харке-Бера. Критерий Бройша-Пагана. Требования к выборке для проведения регрессионного анализа.

### Анализ остатков регрессии

Анализ остатков регрессии необходим для проверки соответствия модели регрессии предположениям и выявления возможных проблем. Основные этапы анализа остатков включают проверку на:

1. **Нормальность остатков**
2. **Гомоскедастичность (постоянная дисперсия остатков)**
3. **Автокорреляцию остатков**
4. **Отсутствие мультиколлинеарности**

### Критерий Дарбина-Уотсона

Критерий Дарбина-Уотсона (DW-тест) используется для обнаружения автокорреляции остатков. Нулевая гипотеза заключается в отсутствии автокорреляции:

$$H_0: \text{Нет автокорреляции}$$

Статистика Дарбина-Уотсона рассчитывается следующим образом:

$$d = \frac{\sum_{t=2}^n (e_t - e_{t-1})^2}{\sum_{t=1}^n e_t^2}$$

где $e_t$ — остатки регрессионной модели в момент времени $t$.

Значение $d$ варьируется от 0 до 4:
- $d \approx 2$ указывает на отсутствие автокорреляции.
- $d < 2$ указывает на положительную автокорреляцию.
- $d > 2$ указывает на отрицательную автокорреляцию.

### Критерий Харке-Бера

Критерий Харке-Бера (JB-тест) проверяет нормальность распределения остатков. Нулевая гипотеза заключается в нормальности остатков:

$$H_0: \text{Остатки нормальны}$$

Статистика Харке-Бера определяется следующим образом:

$$JB = \frac{n}{6} \left(S^2 + \frac{(K - 3)^2}{4}\right)$$

где:
- $n$ — размер выборки,
- $S$ — коэффициент асимметрии остатков,
- $K$ — коэффициент эксцесса остатков.

Если значение JB-статистики велико и превышает критическое значение, нулевая гипотеза отвергается, что указывает на ненормальность распределения остатков.

### Критерий Бройша-Пагана

Критерий Бройша-Пагана (BP-тест) используется для проверки гетероскедастичности (непостоянной дисперсии) остатков. Нулевая гипотеза заключается в гомоскедастичности:

$$H_0: \text{Гомоскедастичность (постоянная дисперсия)}$$

Процедура теста:
1. Оцените регрессию и получите остатки $e_t$.
2. Постройте вспомогательную регрессию: $e_t^2 = \alpha_0 + \alpha_1 x_{1t} + \alpha_2 x_{2t} + \ldots + \alpha_k x_{kt} + u_t$.
3. Рассчитайте статистику BP-теста:

$$BP = n \cdot R^2$$

где $n$ — размер выборки, $R^2$ — коэффициент детерминации вспомогательной регрессии.

Если значение BP-статистики велико и превышает критическое значение, нулевая гипотеза отвергается, что указывает на наличие гетероскедастичности.

### Требования к выборке для проведения регрессионного анализа

Для корректного проведения регрессионного анализа должны выполняться следующие предположения:

1. **Линейность**: Зависимость между зависимой и независимыми переменными должна быть линейной.
2. **Независимость наблюдений**: Наблюдения должны быть независимыми друг от друга.
3. **Нормальность остатков**: Остатки регрессионной модели должны быть нормально распределены.
4. **Гомоскедастичность**: Дисперсия остатков должна быть постоянной.
5. **Отсутствие мультиколлинеарности**: Независимые переменные не должны быть сильно коррелированы между собой.
6. **Отсутствие автокорреляции**: Остатки не должны быть автокоррелированы.

Если данные не соответствуют этим предположениям, могут быть использованы различные методы для коррекции модели, такие как трансформация переменных, регуляризация или использование более сложных моделей.

### Пример

Предположим, у нас есть данные о продажах ($y$) и двух независимых переменных: реклама на телевидении ($x_1$) и реклама в интернете ($x_2$). Мы строим модель множественной регрессии:

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$$

1. **Оценка модели**:
   Используя метод наименьших квадратов, находим $\beta_0, \beta_1, \beta_2$.

2. **Анализ остатков**:
   Проверяем нормальность остатков с помощью критерия Харке-Бера, гомоскедастичность с помощью критерия Бройша-Пагана и автокорреляцию с помощью критерия Дарбина-Уотсона.

3. **Выводы**:
   Если предположения выполняются, делаем выводы о значимости модели и коэффициентов. Если предположения нарушаются, принимаем меры для улучшения модели, такие как трансформация переменных или применение более сложных методов анализа.

[Вернуться к вопросам](#ответы-на-вопросы-по-теории)

## 4.4. Требования к выборке для проведения регрессионного анализа. Нелинейная регрессия. ANCOVA.

### Требования к выборке для проведения регрессионного анализа

Для корректного проведения регрессионного анализа необходимо выполнение ряда предположений и требований к выборке:

1. **Линейность**: Зависимость между зависимой и независимыми переменными должна быть линейной. Это можно проверить с помощью графического анализа (например, диаграммы рассеяния).

2. **Независимость наблюдений**: Наблюдения должны быть независимыми друг от друга. Нарушение этого предположения может привести к неверным выводам.

3. **Нормальность остатков**: Остатки регрессионной модели должны быть нормально распределены. Это предположение можно проверить с помощью графиков нормального распределения остатков или тестов на нормальность (например, теста Харке-Бера).

4. **Гомоскедастичность**: Дисперсия остатков должна быть постоянной. Гомоскедастичность можно проверить с помощью графика остатков против предсказанных значений или с помощью теста Бройша-Пагана.

5. **Отсутствие мультиколлинеарности**: Независимые переменные не должны быть сильно коррелированы между собой. Это предположение проверяется с помощью анализа корреляционной матрицы или расчета показателя VIF (Variance Inflation Factor).

6. **Отсутствие автокорреляции**: Остатки не должны быть автокоррелированы. Это предположение проверяется с помощью критерия Дарбина-Уотсона.

### Нелинейная регрессия

Нелинейная регрессия используется для моделирования зависимостей, которые не являются линейными. Формула нелинейной регрессии может выглядеть следующим образом:

$$
y = f(x_1, x_2, \ldots, x_k; \beta_1, \beta_2, \ldots, \beta_k) + \epsilon
$$

где $f$ — нелинейная функция, зависящая от независимых переменных $x_1, x_2, \ldots, x_k$ и параметров $\beta_1, \beta_2, \ldots, \beta_k$.

#### Пример модели нелинейной регрессии

Полиномиальная регрессия:

$$y = \beta_0 + \beta_1 x + \beta_2 x^2 + \ldots + \beta_k x^k + \epsilon$$

Экспоненциальная регрессия:

$$y = \beta_0 e^{\beta_1 x} + \epsilon$$

Логистическая регрессия:

$$P(y=1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}}$$

### ANCOVA (анализ ковариации)

ANCOVA (Analysis of Covariance) — это метод, который объединяет дисперсионный анализ (ANOVA) и регрессионный анализ для изучения влияния одной или нескольких независимых категориальных переменных на зависимую переменную, при этом контролируя влияние одной или нескольких непрерывных ковариат (ковариационных переменных).

#### Основная модель ANCOVA

$$y_{ij} = \mu + \tau_i + \beta (x_{ij} - \bar{x}) + \epsilon_{ij}$$

где:
- $y_{ij}$ — значение зависимой переменной для $j$-го наблюдения в $i$-й группе,
- $\mu$ — общая средняя,
- $\tau_i$ — эффект $i$-й группы,
- $\beta$ — коэффициент регрессии для ковариаты $x_{ij}$,
- $x_{ij}$ — значение ковариаты для $j$-го наблюдения в $i$-й группе,
- $\bar{x}$ — среднее значение ковариаты,
- $\epsilon_{ij}$ — ошибка.

#### Шаги проведения ANCOVA

1. **Проверка предположений**: Линейность отношения между ковариатой и зависимой переменной, гомоскедастичность и нормальность остатков.
2. **Проведение ANOVA**: Оценка значимости категориальных переменных.
3. **Регрессионный анализ**: Оценка значимости ковариат.
4. **Оценка модели ANCOVA**: Оценка общей модели, включающей как категориальные переменные, так и ковариаты.

### Пример ANCOVA

Предположим, что мы исследуем влияние вида рекламы (телевизионная реклама, интернет-реклама) на продажи, контролируя влияние бюджета рекламы.

1. **Категориальная переменная**: Вид рекламы (телевизионная реклама, интернет-реклама).
2. **Ковариата**: Бюджет рекламы.
3. **Зависимая переменная**: Продажи.

Модель ANCOVA будет выглядеть следующим образом:

$$\text{Продажи} = \mu + \tau_{\text{вид рекламы}} + \beta (\text{Бюджет рекламы} - \bar{\text{Бюджет рекламы}}) + \epsilon$$

В этой модели мы оцениваем, как вид рекламы влияет на продажи, контролируя влияние бюджета рекламы.

ANCOVA позволяет учесть влияние непрерывных переменных и улучшить оценку влияния категориальных переменных, что делает его полезным инструментом для более точного анализа данных в различных областях исследования.

[Вернуться к вопросам](#ответы-на-вопросы-по-теории)
